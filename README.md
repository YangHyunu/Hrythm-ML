![image](https://github.com/user-attachments/assets/4d287a29-c673-442a-a586-e99e42db2cb6)
# HR Analytics Practice Problem  

## 1. í”„ë¡œì íŠ¸ ê°œìš”
- **ë°°ê²½** : ì¤‘ì•™ëŒ€í•™êµ 2024 2í•™ê¸° AIë¥¼ ìœ„í•œ ë¨¸ì‹ ëŸ¬ë‹ ìº¡ìŠ¤í†¤ 
- **ê¸°ê°„**: 2024.11.08 ~ 2024.12.03 (1ê°œì›”)  
- **ì¸ì›**: íŒ€ Hrythm (5ëª…)
  
| ì´ë¦„    | ì—­í•  ë° ê¸°ì—¬                                    |
|---------|-------------------------------------------------|
| ì–‘í˜„ìš°  | ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„¤ê³„, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, Stacking |
| ìœ ë¥´ê²  | ë°ì´í„° ì „ì²˜ë¦¬, LDA/QDA/Logit ëª¨ë¸ ê°œë°œ              |
| ë°•í˜œì›  | EDA, ìë£Œ ì¡°ì‚¬, PPT ì œì‘                         |
| ìš°ë‹¨ë¹„  | RandomForest, SVM, PPT ì œì‘                      |
| ê¹€ì§„ì›  | ìë£Œ ì¡°ì‚¬                                      |

- **í”Œë«í¼**: Analytics Vidhya  
- **ë°ì´í„° ì¶œì²˜**: WNS Analytics Wizard 2018 HR ë°ì´í„°  
- **ëª©í‘œ**:  
  ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ìœ¼ë¡œ ìŠ¹ì§„ ëŒ€ìƒì ì˜ˆì¸¡ ëª¨ë¸ì„ ê°œë°œí•˜ê³ , ê³µì •í•˜ê³  íš¨ìœ¨ì ì¸ ìŠ¹ì§„ í”„ë¡œì„¸ìŠ¤ ìˆ˜ë¦½ì— ê¸°ì—¬í•  ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œ.

## ğŸ¯ ë¬¸ì œ ì •ì˜

- ê¸°ì¡´ ìŠ¹ì§„ ì˜ˆì¸¡ì€ ê´€ë¦¬ì ì£¼ê´€ì— ì˜ì¡´í•´ **í¸í–¥(bias)** ì´ ê°œì…ë  ìˆ˜ ìˆìŒ
- ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ë¡œ **ê³µì •í•˜ê³  ì¼ê´€ëœ ìŠ¹ì§„ íŒë‹¨ ê·¼ê±°**ë¥¼ ì œì‹œí•˜ê³ ì í•¨
  
##  ì—­í•  ë° ê¸°ì—¬

- ì „ì²˜ë¦¬ ë° íŒŒì´í”„ë¼ì¸ ì„¤ê³„: ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ë²”ì£¼í˜• ì¸ì½”ë”© ìë™í™”, ë„ë©”ì¸ ê¸°ë°˜ íŒŒìƒ ë³€ìˆ˜ ì„¤ê³„
- RFECV ë¥¼ í™œìš©í•œ ë³€ìˆ˜ ì„ íƒ
- Optuna ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œìŠ¤í…œ ì„¤ê³„ ë° êµ¬í˜„
- XGBoost, LGBM, CatBoost, LDA ê¸°ë°˜ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” êµ¬ì¡° ì„¤ê³„ ë° ê²€ì¦
---

## 2. ë°ì´í„° ì´í•´ ë° ì£¼ìš” ì´ìŠˆ

- **ë°ì´í„° íŠ¹ì„±**
  - ì´ 54,000ì—¬ ëª…ì˜ ì§ì› ë°ì´í„°, ë‹¤ì¤‘ ë²”ì£¼í˜• ë° ì—°ì†í˜• ë³€ìˆ˜ í˜¼ì¬
  - **ìŠ¹ì§„ì ë¹„ìœ¨ ì•½ 11%**ë¡œ,  í´ë˜ìŠ¤ ë¶ˆê· í˜• ì¡´ì¬
  - ë³€ìˆ˜ ê°„ ë‹¤ì¤‘ê³µì„ ì„± ë° íƒ€ê²Ÿê³¼ì˜ ë‚®ì€ ì„ í˜• ìƒê´€ì„±ìœ¼ë¡œ SMOTE ë“± ìƒ˜í”Œë§ ê¸°ë²• ì‚¬ìš©
  

- **ë¶„ì„ ì£¼ìš” ì´ìŠˆ**
  - ì „í†µì ì¸ **ìƒê´€ë¶„ì„(Pearson)** ë° **PCA** ì ìš© ê²°ê³¼ íƒ€ê²Ÿê³¼ ê°•í•œ ì„ í˜• ì—°ê´€ì„± ë¯¸ì•½
  - ì—¬ëŸ¬ íŒŒìƒ ë³€ìˆ˜ë¥¼ ë„ì¶œí–ˆìœ¼ë‚˜ ì˜ˆì¸¡ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ëª»í•¨
  - í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ í•´ê²°í•˜ê¸° ìœ„í•´ SMOTEë¥¼ ì ìš©í–ˆìœ¼ë‚˜, ë‹¨ì¼ ëª¨ë¸ì˜ ê²½ìš° íƒ€ê²Ÿ íŠ¹ì„±ì´ì™¸ì˜ íŠ¹ì„±ë“¤ ë˜í•œ ë¶ˆê· í˜•ì´ ì‹¬í•´ ê³¼ì í•©ì´ ë°œìƒí•˜ì˜€ìŒ
  - â†’ **ë¹„ì„ í˜• ëª¨ë¸ ë° ì•™ìƒë¸” êµ¬ì¡° í•„ìš”ì„± ëŒ€ë‘**

### ì‹œê°í™”

![image](https://github.com/user-attachments/assets/39be2097-f2a2-4aa8-b9ac-f8302c931665)

**PCA ê²°ê³¼** : 2ê°œì˜ ì£¼ì„±ë¶„ìœ¼ë¡œ ì•½ 85% ì´ìƒì˜ ì´ë³€ë™ì„ ì„¤ëª…í•  ìˆ˜ ìˆì—ˆìœ¼ë‚˜
  - ê° ì£¼ì„±ë¶„ì—ì„œì˜ ì¤‘ìš” ì„±ë¶„ë“¤ì´ 'age', 'length_of_service', 'average_training_score' ë“±ìœ¼ë¡œ, íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ê°€ ë‚®ìŒ
  - ëŒ€íšŒ ì´í›„ ì‚¬í›„ì ìœ¼ë¡œ ì˜¤í† ì¸ì½”ë”ë¥¼ í†µí•´ ë¹„ì„ í˜• íŠ¹ì„±ì„ ì¶”ì¶œí•œ í›„ ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ì´ ìƒë‹¹íˆ í–¥ìƒë˜ì—ˆìŒ

![image](https://github.com/user-attachments/assets/235a3622-ed20-4801-a449-44684a319818)
*ë³€ìˆ˜ ê°„ ë‚®ì€ ìƒê´€ê³„ìˆ˜: ë‹¨ì¼ ë³€ìˆ˜ ê¸°ë°˜ ì˜ˆì¸¡ì˜ í•œê³„*

---

## 3. ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§

- ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ë²”ì£¼í˜• ë³€ìˆ˜ Encoding (One-hot, Label)
- 'education', 'length_of_service'ì— ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ì—¬ ê°ê° IterativeImputer, SimpleImputerë¡œ ì²˜ë¦¬
- `education`, `department`, `age`, `service_year` ë“± ë„ë©”ì¸ ê¸°ë°˜ íŒŒìƒ ë³€ìˆ˜ ìƒì„±
  
**RFECV, Stepwise Feature Selection**ìœ¼ë¡œ ê° ëª¨ë¸ì— ë§ëŠ” ìµœì ì˜ í”¼ì²˜ë¥¼ ì„ íƒí•˜ì˜€ìŒ

- **RFECV:**  
  - Recursive Feature Elimination with Cross-Validationì„ í†µí•´ ìµœì  í”¼ì²˜ ì„ íƒ  
  - ìµœì  í”¼ì²˜ ìˆ˜ë¥¼ ì°¾ê¸° ìœ„í•´ 5-fold CVë¥¼ ì ìš©
  - ë¶€ìŠ¤íŒ…ê³¼ ë°°ê¹…ëª¨ë¸ì—ë§Œ ì ìš©
    
![image](https://github.com/user-attachments/assets/6c78114a-84ce-4986-b303-8a04ace2195c)

  
- **Stepwise Selection:**  
  - ì „ì§„ì„ íƒë²•ê³¼ í›„ì§„ì„ íƒë²•ì„ í†µí•´ ìµœì  í”¼ì²˜ ì„ íƒ  
  - AIC/BICë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì  í”¼ì²˜ ìˆ˜ë¥¼ ì°¾ìŒ
![image](https://github.com/user-attachments/assets/be691644-49b4-4dbf-911e-7c8f7e6dd165)

<details>
  <summary><strong>ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì½”ë“œ ë³´ê¸°</strong></summary>

```python
import pandas as pd
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤ ì •ì˜
class Preprocessor(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.imputer_rating = IterativeImputer(max_iter=10, random_state=42)
        self.imputer_education = SimpleImputer(strategy="most_frequent")
        self.scaler = StandardScaler()
        self.education_map = {'Below Secondary': 0, "Bachelor's": 1, "Master's & above": 2}

    def fit(self, X, y=None):
        self.imputer_rating.fit(X[['previous_year_rating']])
        self.imputer_education.fit(X[['education']])
        return self

    def transform(self, X):
        X[['previous_year_rating']] = self.imputer_rating.transform(X[['previous_year_rating']])
        X[['education']] = self.imputer_education.transform(X[['education']])
        X['education'] = X['education'].map(self.education_map)
        X['region'] = X['region'].str.replace('region_', '').astype(int)
        X.rename(columns={
            "employee_id": "id",
            "KPIs_met >80%": "KPIs_over_0.8",
            "awards_won?": "awards_won"
        }, inplace=True)
        for column in X.columns:
            if column in object_columns:
                dummies = pd.get_dummies(X[column], drop_first=True, prefix=f"{column}")
                dummies = dummies.astype(int)
                X = pd.concat([X, dummies], axis=1)
                X.drop(columns=column, inplace=True)
        return X

# íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜
def create_pipeline():
    pipeline = Pipeline([
        ('preprocessor', Preprocessor())
    ])
    return pipeline

# ì „ì²˜ë¦¬ ì‹¤í–‰
pipeline = create_pipeline()
train = pipeline.fit_transform(train)
test = pipeline.transform(test)
```
</details>

### 3.1 í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- **Optuna**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ML ëª¨ë¸(lda,qda,rf,logit,LGBM, XGB, CatBoost, RF ë“±)ì„ ê°œë³„ í•™ìŠµ í›„ í‰ê°€í–ˆìŒ. ì—¬ëŸ¬ ëª¨ë¸ì˜ ì¡°í•©ì„ ì‹œë„í•˜ì—¬ ì•™ìƒë¸” ë° ìŠ¤íƒœí‚¹ ì ìš©
<details>
  <summary><strong>Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì½”ë“œ ë³´ê¸°</strong></summary>

<br>

```python
import optuna
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.svm import SVC


class OptunaOptimizer:
    def __init__(self, models, train_X, train_y, metric, rfe=None):
        self.models = models
        self.train_X = train_X
        self.train_y = train_y
        self.metric = metric
        self.rfe = rfe if rfe else {model_name: train_X.columns.tolist() for model_name in models}
        self.studies = {}
        self.best_models = {}
        self.scaler = StandardScaler()

    def objective(self, trial, model_class, model_name):
        selected_features = self.rfe[model_name]
        train_X_filtered = self.train_X[selected_features]

        if model_class == LogisticRegression:
            param = {
                'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),
                'C': trial.suggest_float('C', 1e-4, 1e4, log=True),
                'solver': trial.suggest_categorical('solver', ['saga']),
                'l1_ratio': trial.suggest_float('l1_ratio', 0, 1, log=False)
            }
        elif model_class == XGBClassifier:
            param = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                'gamma': trial.suggest_float('gamma', 0, 5),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0)
            }
        elif model_class == LGBMClassifier:
            param = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                'num_leaves': trial.suggest_int('num_leaves', 20, 150),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0)
            }
        elif model_class == RandomForestClassifier:
            param = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 20),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),
                'bootstrap': trial.suggest_categorical('bootstrap', [True, False])
            }
        elif model_class == LinearDiscriminantAnalysis:
            solver = trial.suggest_categorical('solver', ['svd', 'lsqr', 'eigen'])
            param = {'solver': solver}
            if solver in ['lsqr', 'eigen']:
                param['shrinkage'] = trial.suggest_categorical('shrinkage', ['auto', None])
                if param['shrinkage'] is None:
                    param['shrinkage'] = trial.suggest_float('shrinkage_float', 0.0, 1.0)
        elif model_class == QuadraticDiscriminantAnalysis:
            param = {
                'reg_param': trial.suggest_float('reg_param', 0.0, 1.0)
            }
        elif model_class == SVC:
            kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])
            param = {
                'C': trial.suggest_float('C', 1e-4, 1e4, log=True),
                'kernel': kernel,
                'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])
            }
            if kernel == 'poly':
                param['degree'] = trial.suggest_int('degree', 2, 5)
                param['coef0'] = trial.suggest_float('coef0', 0.0, 1.0)
            if kernel == 'sigmoid':
                param['coef0'] = trial.suggest_float('coef0', 0.0, 1.0)
        elif model_class == CatBoostClassifier:
            param = {
                'iterations': trial.suggest_int('iterations', 100, 500),
                'depth': trial.suggest_int('depth', 4, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-1, 1e2, log=True),
                'border_count': trial.suggest_int('border_count', 1, 255),
                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
                'random_strength': trial.suggest_float('random_strength', 0.0, 1.0),
                'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),
                'od_wait': trial.suggest_int('od_wait', 10, 50)
            }

        train_split_X, val_X, train_split_y, val_y = train_test_split(train_X_filtered, self.train_y, test_size=0.2, random_state=42, stratify=self.train_y)
        train_split_X = self.scaler.fit_transform(train_split_X)
        val_X = self.scaler.transform(val_X)
        model = model_class(**param)
        model.fit(train_split_X, train_split_y)

        pred_y = model.predict(val_X)
        if self.metric == 'f1_score':
            score = f1_score(val_y, pred_y, average='macro')
        elif self.metric == 'accuracy_score':
            score = accuracy_score(val_y, pred_y)
        elif self.metric == 'roc_auc_score':
            score = roc_auc_score(val_y, model.predict_proba(val_X)[:, 1])
        return score

    def optimize(self, n_trials=100):
        for model_name, model_class in self.models.items():
            print(f"Starting hyperparameter tuning for {model_name}")
            study = optuna.create_study(direction='maximize')
            study.optimize(lambda trial: self.objective(trial, model_class, model_name), n_trials=n_trials)
            self.studies[model_name] = study
            print(f"Best parameters for {model_name}: {study.best_params}")
            print(f"Best {self.metric} for {model_name}: {study.best_value}")
            print(study)
            
    def train_best_models(self):
        for model_name, model_class in self.models.items():
            selected_features = self.rfe[model_name]
            train_X_filtered = self.train_X[selected_features]
            best_params = self.studies[model_name].best_params
            best_model = model_class(**best_params)
            
            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
            train_split_X, val_X, train_split_y, val_y = train_test_split(train_X_filtered, self.train_y, test_size=0.2, random_state=42, stratify=self.train_y)
            train_split_X = self.scaler.fit_transform(train_split_X)
            val_X = self.scaler.transform(val_X)
            
            best_model.fit(train_split_X, train_split_y)
            self.best_models[model_name] = best_model

            pred_y = best_model.predict(val_X)
            f1 = f1_score(val_y, pred_y, average='binary')
            print(f"F1 score with best parameters for {model_name}: {f1}")


# Optimizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ìµœì í™” ìˆ˜í–‰
optimizer = OptunaOptimizer(models, train_X, train_y, 'f1_score', rfe)
optimizer.optimize(n_trials=30)
optimizer.train_best_models()
```
</details>

---

## 4. ëª¨ë¸ë§ ì „ëµ ë° ì„±ëŠ¥ ë¹„êµ

### 4.1 ë‹¨ì¼ ëª¨ë¸ ê¸°ë°˜ 

| Model            | Val (SMOTE) | Test (ë¹„ê³µê°œ) | ì„±ëŠ¥ ê°ì†Œí­ |
|------------------|-------------|----------------|--------------|
| **LDA**          | 0.7200      | 0.4457         | â†“ 0.2743     |
| **QDA**          | 0.6806      | 0.4000         | â†“ 0.2806     |
| **Logistic Reg.**| 0.6158      | 0.3086         | â†“ 0.3072     |
| **KNN**          | 0.6200      | 0.2813         | â†“ 0.3387     |
| **Naive Bayes**  | 0.6100      | 0.2880         | â†“ 0.3220     |
| **SVM**          | 0.7400      | 0.2900         | â†“ 0.4500     |
| **Random Forest**| 0.9900      | 0.4200         | â†“ 0.5700     |

![image](https://github.com/user-attachments/assets/b785a596-f996-472f-8ea7-74542dd376d3)

- **í›ˆë ¨-ê²€ì¦ ê°„ ê´´ë¦¬** í˜„ìƒ â†’ ê³¼ì í•© ë°œìƒ
- **ë¹„ê³µê°œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸**ì—ì„œ F1-score 0.45 ë‚´ì™¸
  - ë¶€ìŠ¤íŒ… ëª¨ë¸ì´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜ ì—­ì‹œ validation ì—ì„œëŠ” 0.7~ 0.8ì˜ f1-scoreë¥¼ ë³´ì—¬ì£¼ë©° í›ˆë ¨ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸(ë¹„ê³µê°œ)ë°ì´í„°ê°€ ìƒë‹¹íˆ ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°€ì§€ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
---

### 4.2 Soft / Hard Voting ê²°ê³¼
- ë¶€ìŠ¤íŒ…ê³¼ ë°°ê¹… ëª¨ë¸ì´ ê·¸ ìì²´ë¡œ ë³´íŒ…ëª¨ë¸ì¸ ê²ƒì„ ì•Œì§€ë§Œ, ë‹¤ì–‘í•œ íŠ¹ì„±ì„ í•™ìŠµí•œ ëª¨ë¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë³´íŒ…ëª¨ë¸ì„ ë§Œë“¤ì–´ ì‹œë„
![image](https://github.com/user-attachments/assets/b328f3ac-9c56-4d39-9f3c-2ba2edc7b4bd)

| Voting Method | Models | F1-Score |
|---------------|--------|----------|
| **Soft Voting** | xgb, lgbm, cat, lda | **0.4884** |
| Soft Voting | xgb, lgbm, rf, cat, lda, qda, logit | 0.4852 |
| Hard Voting | xgb, lgbm, cat, lda | 0.4795 |
| Hard Voting | xgb, lgbm, rf, cat, lda, qda, logit | 0.4442 |

> Soft Votingì´ ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„±ëŠ¥ í™•ë³´
    - Hard Votingì˜ ê²½ìš°  xgb, lgbm, catê³¼ ê°™ì€ ë¶€ìŠ¤íŒ… ëª¨ë¸ë“¤ì´ ê³µí†µì ìœ¼ë¡œ ê³¼ì í•©í•˜ì—¬ ê°€ì¥ ì¢‹ì§€ ëª»í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.
ë”°ë¼ì„œ ì•™ìƒë¸” ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ë‹¤ì–‘í•œ íŠ¹ì„±ì„ í•™ìŠµí•œ ëª¨ë¸ë“¤ì„ ì¡°í•©í•˜ì—¬ ì•™ìƒë¸” ëª¨ë¸ì„ ë§Œë“¤ê³ , ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”ì„ ì ìš©í•˜ê¸°ë¡œ í–ˆë‹¤.


### 4.3 Stacking Ensemble ê²°ê³¼
> Stacking Ensemble  
  - ìœ„ ëª¨ë¸ í•™ìŠµ ê²°ê³¼, ë¶€ìŠ¤íŒ… ê³„ì—´ì´ ì•„ë‹Œ ëª¨ë¸ë“¤ì€ ê³¼ì†Œì í•© ë˜ëŠ” ê³¼ì í•© ë¬¸ì œë¥¼ ë³´ì˜€ìœ¼ë©°, ê° ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” í”¼ì²˜ë“¤ì´ ì„œë¡œ ë‹¤ë¦„ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŒ.  
ì´ëŸ¬í•œ ì ì„ ê³ ë ¤í•˜ì—¬, ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì„±ì„ í•™ìŠµí•œ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©í•˜ëŠ” ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë” ê°•ë ¥í•˜ê³  ì¼ë°˜í™”ëœ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆë‹¤ê³  íŒë‹¨í•¨.  
ì¦‰, ê° ë² ì´ìŠ¤ ëª¨ë¸ì´ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ ì˜ˆì¸¡ ì •ë³´ë¥¼ ë©”íƒ€ ëª¨ë¸ì´ ì¢…í•©í•¨ìœ¼ë¡œì¨, ê°œë³„ ëª¨ë¸ì˜ ì•½ì ì„ ë³´ì™„í•˜ê³  ë°ì´í„° ë¶„í¬ ì°¨ì´ ë° ê³¼ì í•© ë¬¸ì œë¥¼ ì¤„ì¼ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•¨.

![image](https://github.com/user-attachments/assets/53d95b54-29e1-40a0-8fdb-5ef5e77ee455)


![image](https://github.com/user-attachments/assets/dcd6242a-bafe-4d3b-b91d-2788e16c5813)

> ë©”íƒ€ ëŸ¬ë„ˆë¡œ **LDA ì‚¬ìš© ì‹œ ìµœê³  ì„±ëŠ¥**ì„ ê¸°ë¡  
  - ë³¸ ê²°ê³¼ì—ì„œëŠ” ë‹¨ìˆœ íšŒê·€ ëª¨í˜•ì´ë‚˜ ë‹¨ìˆœ ëª¨ë¸ ëŒ€ì‹  ê°•ë ¥í•œ ë¶€ìŠ¤íŒ… ê³„ì—´ ëª¨ë¸(XGB, LGBM, CatBoost ë“±)ì„ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ê³ ,  
  ë©”íƒ€ ëª¨ë¸ë¡œ ì„ í˜• ê¸°ë°˜ì˜ LDAë¥¼ ì ìš©í•˜ì˜€ì„ ë•Œ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í•¨.  
ì´ëŠ” ë¶€ìŠ¤íŒ… ëª¨ë¸ë“¤ì´ í•™ìŠµí•œ ë‹¤ì–‘í•œ ë¹„ì„ í˜• íŠ¹ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©í•œ í›„, LDAê°€ ì´ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ì¡°í•©í•˜ì—¬ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•œ ê²°ê³¼ë¡œ í•´ì„í•  ìˆ˜ ìˆìŒ.  
íŠ¹íˆ, ì¼ë°˜ì ìœ¼ë¡œ ë©”íƒ€ ëŸ¬ë„ˆë¡œ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ ë§ì´ ì‚¬ìš©í•˜ì§€ë§Œ, ì´ë²ˆ ê²°ê³¼ì—ì„œëŠ” LDAê°€ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°,  
ë©”íƒ€ ëŸ¬ë„ˆë¡œ QDAë¥¼ ì‚¬ìš©í•œ ê²½ìš°ì—ëŠ” ë² ì´ìŠ¤ ëª¨ë¸ë“¤ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì— ë”°ë¼ ì„±ëŠ¥ ì°¨ì´ê°€ í¬ê²Œ ë‚˜íƒ€ë‚¨. ëŒ€íšŒ ì‚¬í›„ ì˜¤í† ì¸ì½”ë”ë¥¼ í†µí•´ ë¹„ì„ í˜• íŠ¹ì„±ì„ ì¶”ì¶œí•œ ëª¨ë¸ì—ì„œ ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í•¨.

---



## 5. ê²°ê³¼ ë° ì¸ì‚¬ì´íŠ¸
![image](https://github.com/user-attachments/assets/dd0c9fcc-7cb2-4264-bc14-d8fd1f182fb5)

- í•µì‹¬ ë³€ìˆ˜: `kpi_met > 80%`, `awards_won`, `department`  
- ìƒëŒ€ì ìœ¼ë¡œ ë¹„ì¤‘ ë‚®ì€ ë³€ìˆ˜: `length_of_service`, `education`, `no_of_trainings`

- **ê³¼ì í•©**ë³´ë‹¤ **ë°ì´í„° ë¶„í¬ ì°¨ì´**ê°€ ë” í° ì„±ëŠ¥ ì €í•˜ ìš”ì¸ì´ ë  ìˆ˜ ìˆìŒ
- ë‹¤ì–‘í•œ ë¶„í¬ë¥¼ í•™ìŠµí•œ ëª¨ë¸ ì¡°í•©ê³¼ **ìŠ¤íƒœí‚¹ ì „ëµ**ì´ ì¼ë°˜í™” ì„±ëŠ¥ í™•ë³´ì— í•µì‹¬
- ë‹¨ìˆœí•œ ëª¨ë¸ íŠœë‹ë³´ë‹¤ **êµ¬ì¡°ì  ì„¤ê³„ì˜ ì¤‘ìš”ì„±**ì„ ì²´ê°
- í–¥í›„ì—ëŠ” êµ°ì§‘ ê¸°ë°˜ ë¶„ì„ì´ë‚˜ ë¹„ì§€ë„ í•™ìŠµì„ ê²°í•©í•œ **ë°ì´í„° ê·¸ë£¹ë³„ í•™ìŠµ ì „ëµ**ì´ í•„ìš”

---

## 6. íšŒê³  ë° ê°œì„ ì 

- í”„ë¡œì íŠ¸ ì´ˆê¸°ì—ëŠ” ì£¼ë¡œ ëª¨ë¸ í•™ìŠµ, ì „ì²˜ë¦¬, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë“± **ì„±ëŠ¥ ìµœì í™” ì¤‘ì‹¬ì˜ ê¸°ìˆ ì  ê¸°ì—¬**ì— ì§‘ì¤‘í•˜ì˜€ìŒ.
- ê·¸ëŸ¬ë‚˜ ì´ë¡œ ì¸í•´ ë°ì´í„° ê¸°ë°˜ EDAë¥¼ í†µí•´ **ì¸ì‚¬ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ ë„ì¶œì´ë‚˜ í•µì‹¬ ë³€ìˆ˜ ë¶„ì„**ì€ ìƒëŒ€ì ìœ¼ë¡œ ë¶€ì¡±í•œ ê²°ê³¼ë¥¼ ì´ˆë˜í•¨.
- ëª¨ë¸ ì„±ëŠ¥ì€ ì¼ì • ìˆ˜ì¤€ ì´ìƒ í™•ë³´í•˜ì˜€ìœ¼ë‚˜, **ì‹¤ì œ ì¸ì‚¬ ì „ëµ ìˆ˜ë¦½ì— í™œìš© ê°€ëŠ¥í•œ í•´ì„ ì¤‘ì‹¬ ë¶„ì„ì´ ë¯¸í¡**í•˜ë‹¤ëŠ” ì ì„ ì‚¬í›„ì— ì¸ì§€í•˜ê²Œ ë˜ì—ˆìŒ.
- ì´ì— ë”°ë¼ í”„ë¡œì íŠ¸ ì¢…ë£Œ í›„ ë‹¤ìŒê³¼ ê°™ì€ ë³´ì™„ì„ ì§„í–‰í•¨:
  - PCA ê¸°ë°˜ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ **AutoEncoderë¥¼ í™œìš©í•œ ë¹„ì„ í˜• ì ì¬ íŠ¹ì„± ì¶”ì¶œ** ìˆ˜í–‰
  - **Kibana ê¸°ë°˜ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ**ë¥¼ êµ¬ì¶•í•˜ì—¬ ë³€ìˆ˜ë³„ ì˜í–¥ë ¥ê³¼ ë¶„í¬ë¥¼ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•˜ê³  ì¡°ì§ ì°¨ì›ì˜ í™œìš© ê°€ëŠ¥ì„±ì„ í™•ì¸
- ì´ ê³¼ì •ì„ í†µí•´ ë‹¨ìˆœí•œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë„˜ì–´ì„œ, **ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ì— ê¸°ì—¬í•˜ëŠ” ë°ì´í„° í•´ì„ ì—­ëŸ‰**ì˜ ì¤‘ìš”ì„±ì„ ì²´ê°í•¨.

---


## ğŸ›  ê¸°ìˆ  ìŠ¤íƒ

- Python, Scikit-learn, Pandas, NumPy  
- XGBoost, LightGBM, CatBoost, RF, QDA, LDA  
- SMOTE, Optuna, RFECV  
- Matplotlib, Seaborn

---

## ğŸ“ ì°¸ê³  ìë£Œ
- ğŸ“‚ [ì‚¬í›„ ëŒ€ì‹œë³´ë“œ ë° EDA ë³´ê¸°](./ëŒ€ì‹œë³´ë“œ_EDA.pdf)
- ğŸ“‚ [ìµœì¢… ë³´ê³ ì„œ PDF ë³´ê¸°](./HR_Project.pdf)
- ğŸ“‚ [ìµœì¢… ì½”ë“œ](./last_version.ipynb)
